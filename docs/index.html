<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Data Science Capstone</title>
  <meta name="description" content="">
  <meta name="author" content="Chris Walsh">
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <link rel="stylesheet" href="libraries/frameworks/revealjs/css/reveal.min.css">
  <link rel="stylesheet" href="libraries/frameworks/revealjs/css/theme/night.css" id="theme">
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" id="theme">
  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->  
</head>
<body>
  <div class="reveal">
    <div class="slides">
      <section class='' data-state='' id='slide-1'>
  
  <style>
.reveal p {
    font-size: .75em;
}

.reveal small {
    width: 500px;
}

.reveal .slides {
    text-align: left;
}

.reveal .roll {
    vertical-align: text-bottom;
}

code {
    color: red;
}

.reveal pre code { 
     height: 250px;
}

.reveal section img { 
  background:none; 
  border:none; 
  box-shadow:none; 
  }
  
</style>

<h2>The Use Case</h2>

<p>In today&#39;s mobile world, Natural Langague Processing (NLP) is all around us. This Data Science Capstone, administered in concert with <strong>SwiftKey</strong>, is focused on the development of an NLP model capable of predicting the next word in a text string based on the provided SwiftKey <a href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip">English language corpus</a>. 
<br>
<br></p>

<h3>The Links</h3>

<p>Several products were developed to support this Capstone. They can be found below:</p>

<ol>
<li><p>The Milestone Report: <a href="http://rpubs.com/cwalsh/300582">link here</a></p></li>
<li><p>The Next Word Prediction Shiny application: <a href="https://cwalsh.shinyapps.io/data_science_capstone_next_word_prediction/">link here</a></p></li>
<li><p>The Presentation: <a href="https://cwalsh.shinyapps.io/data_science_capstone_next_word_prediction/">link here</a></p></li>
<li><p>The Code: <a href="https://github.com/chwalsh/10-JHU-Capstone">link here</a></p></li>
</ol>

</section>
<section class='' data-state='' id='slide-2'>
  <h2>The Algorithm</h2>
  <p>Several steps were taken in order to develop the underlying NLP model:</p>

<ul>
<li><p>The Corpus was ingested and cleansed using <code>tm</code> and <code>stringi</code>; this process included: converting characters to a standard encoding;  transforming to lowercase; and stripping punctuation, numbers, and excess whitespace </p></li>
<li><p>The Corpus was tokenized into N-Grams using <code>RWeka</code>; for the purpose of this model, UniGrams, BiGrams, TriGrams, and 4-Grams were developed</p></li>
<li><p>The <strong>Good-Turing discount</strong> was calculated based on N-Gram frequencies to smooth the eventual model<a href="http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures/smoothing+backoff.pdf"><sup>1</sup></a></p></li>
<li><p>A simple <strong>Katz Backoff model</strong> was developed based on these adjusted N-Gram frequencies to estimate the probability of each potential next word<a href="http://www.cs.cornell.edu/courses/cs4740/2014sp/lectures/smoothing+backoff.pdf"><sup>2</sup></a></p></li>
<li><p>To optimize for performance on the Shiny server, the underlying N-Gram frequency tables were pruned; both <strong>cutoff</strong> and <strong>weighted difference</strong> methods were tested, with better model performance (12.53% accuracy) at the desired frequency <code>data.table</code> size observed via the <strong>cutoff</strong> method<a href="https://pdfs.semanticscholar.org/6c8c/dff44ef74915a0276a7e1aba939eae7eefa7.pdf"><sup>3</sup></a></p></li>
</ul>

</section>
<section class='' data-state='' id='slide-3'>
  <h2>The App</h2>
  <p>This algorithm was then deployed via a <a href="https://cwalsh.shinyapps.io/data_science_capstone_next_word_prediction/">Shiny app</a>. When text is input into the box, the Model Output tab provides three unique ways to interact with the model output:</p>

<ol>
<li><p>The next word with the highest probability is appended directly onto the input text to display the full expected text string</p></li>
<li><p>The three next words with the highest probabilities are displayed; think of this as the three predictions typically provided during text input on a mobile device</p></li>
<li><p>A WordCloud is displayed based on the probabilities of the top 100 most likely next words</p></li>
</ol>

<p><img src="figures/NLP.png" title="plot of chunk image" alt="plot of chunk image" height="400px" style="display: block; margin: auto;" /></p>

</section>
<section class='' data-state='' id='slide-4'>
  <h2>Next Steps</h2>
  <ul>
<li><p>As the intial exploratory analysis shows, <em>context matters</em>; N-Gram frequencies differ across blog, twitter, and news corpora. While all three sources are consolidated in the underlying model, greater accuracy could potentially be gained by dynamically selecting corpora sources based on application.</p></li>
<li><p>80% of the overall corpus was initially used to train the model, however this resulted in final frequency tables of approximately <strong>800 MB</strong>. Memory considerations required pruning down to approximately <strong>12 MB</strong> to deploy at the desired speed on a Shiny server. While some testing was performed to select a pruning method, additional techniques can be explored to further minimize accuracy loss.</p></li>
<li><p>Good-Turing discounting was implemented to help smooth the model as it outperformed a simple absolute discount initially used; additional methods, including Kneser-Key, could potentially be explored to further refine model accuracy.</p></li>
<li><p>The app could be updated to dynamically update the underlying frequency tables based on user input; this would allow the model to &quot;learn&quot; from repeated use.</p></li>
</ul>

</section>
    </div>
  </div>
</body>
  <script src="libraries/frameworks/revealjs/lib/js/head.min.js"></script>
  <script src="libraries/frameworks/revealjs/js/reveal.min.js"></script>
  <script>
  // Full list of configuration options available here:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,
    theme: Reveal.getQueryHash().theme || 'night', 
    transition: Reveal.getQueryHash().transition || 'convex', 
    dependencies: [
    // Cross-browser shim that fully implements classList -
    // https://github.com/eligrey/classList.js/
      { src: 'libraries/frameworks/revealjs/lib/js/classList.js', condition: function() { return !document.body.classList;}},
      // Zoom in and out with Alt+click
      { src: 'libraries/frameworks/revealjs/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      // Speaker notes
      { src: 'libraries/frameworks/revealjs/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
      // Remote control your reveal.js presentation using a touch device
      //{ src: 'libraries/frameworks/revealjs/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
      ]
  });
  </script>  <!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
 

</html>