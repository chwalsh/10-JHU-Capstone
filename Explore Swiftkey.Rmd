---
title: "Swiftkey Exploratory Analysis"
author: "Chris Walsh"
date: "August 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tm)
require(tidyr)
require(dplyr)
require(ggplot2)
require(slam)
require(RWeka)
```

## Synopsis

The goal of this analysis is briefly present the results of the intial exploration of the provided English swiftkey text corpus. In order to prepare for predictive modeling, the data has been loaded, preprocessed, and explored to identify key features and gain an understanding of the text.

## Data Load

To begin the assessment, the data was read in via file connections. All files were initally read in using the `"rt" mode`, however intial exploration showed that this resulted in a warning for `en_US.news.txt` and a suspiciously small file size. Switching the mode to `"rb" mode` rectified the problem.

```{r load, cache= TRUE}
twitter.con <- file("raw data/final/en_US/en_US.twitter.txt", "r") 
blogs.con <- file("raw data/final/en_US/en_US.blogs.txt", "r") 
news.con <- file("raw data/final/en_US/en_US.news.txt", "rb") 

US.twitter <- readLines(twitter.con, skipNul = TRUE) 
US.blogs <- readLines(blogs.con, skipNul = TRUE) 
US.news <- readLines(news.con, skipNul = TRUE) 
doc.list <- list(blog = US.blogs, twitter = US.twitter, news = US.news)

close(twitter.con)
close(blogs.con)
close(news.con)
rm(twitter.con)
rm(blogs.con)
rm(news.con)
```

This resulted in a sizeable volume of data that could not be analyzed in full in a timely manner. To address this, the data was partitioned to create a `5%` sample for corpus creation and exploratory analysis.

```{r partition, cache=TRUE}
set.seed(522)
percent <- 0.05
random <- lapply(doc.list, function (x) rbinom(x, 1, percent))

sample.list <- list(blog = NA, twitter = NA, news = NA)

for (i in 1:length(doc.list)) {
  sample.list[[i]] <- doc.list[[i]][random[[i]] == 1]
}
```


## Create Corpus

Once a sample has been created, a Corpus was created via the `tm` package.

```{r preprocess, cache=TRUE}
docs <- VCorpus(VectorSource(sample.list))

docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removeNumbers)
doc.ns <- tm_map(docs, removeWords,
                 stopwords(kind="en"))
```

Standard preprocessing steps - to include the removal of punctuation, the transformation to lowercase characters, the removal of white space, and the removal of numbers were also completed here. Text analytics projects often also remove stop words, however given that our final model will include the prediction of stop words, this step was not performed on the main Corpus. A separate Corpus was created without stop words to facilitate exploratory analysis.

Once created, a Term Document Matrix was construced to facilitate exploratory analysis. As a first step, this full matrix was used to provide a total word count across each of the three provided sources: blog, twitter, news. This, coupled with a sample line count from the previously created `sample.list`, provides a feel for the volume of text.

```{r explore, cache=TRUE, fig.align="center"}
tdm <- TermDocumentMatrix(doc.ns)

word.count <- colSums(as.matrix(tdm))
names(word.count) <- c("blog", "twitter", "news")
count.plot <- data.frame(source=names(word.count), word.count=word.count, 
                         lines = sapply(sample.list, length))
count.plot$source <- factor(count.plot$source,
                        levels=count.plot$source[order(count.plot$word.count,
                                                   decreasing=TRUE)])
count.plot <- gather(count.plot, type, count, -source)


ggplot(count.plot, aes(x=source, y=count, fill = type)) + geom_bar(stat="identity", position = "dodge") + 
  scale_y_log10() + xlab("Entries in Corpus") + ylab("Count") + theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())
```

The results are relatively intuitive; although twitter provides substantially more lines, the overall word counts across the three samples are comparable due to twitter's character limit.

The Term Document Matrix can also provided a basic plot of the frequency of uni-grams (words) contained within the sample. In this case, we used the Corpus without stop words to see what other kinds of words are contained within the sample.

```{r explore2, cache=TRUE, echo=FALSE, fig.align="center"}
tdm <- removeSparseTerms(tdm, 0.9999)

tdm.freq <- sort(row_sums(tdm, na.rm = T), decreasing=TRUE)
tdm.freq <- data.frame(word=names(tdm.freq), freq=tdm.freq)

uni.plot <- as.data.frame(head(tdm.freq, 10))
uni.plot$word <- factor(uni.plot$word,
                      levels=uni.plot$word[order(uni.plot$freq,
                                               decreasing=TRUE)])

ggplot(uni.plot, aes(x=word, y=freq)) + geom_bar(stat="identity") + 
  xlab("Words in Corpus (Stop Words Excluded)") + ylab("Count") + theme_minimal()
```

## Tokenize and Plot

This analysis can be extended to bi-grams. The frequency plot was repeated using a bi-gram tokenizer on the Corpus that includes stop words. The most frequent word pairings identified are logical for the English language, which is a good sign.

```{r bi-tokenize, cache =TRUE, fig.align="center"}
BiTokens <- function(x) {NGramTokenizer(x, Weka_control(min = 2, max = 2))}
TriTokens <- function(x) {NGramTokenizer(x, Weka_control(min = 3, max = 3))}
QuadTokens <- function(x) {NGramTokenizer(x,Weka_control(min = 4, max = 4))}


tdm2 <- TermDocumentMatrix(docs, control=list(tokenize=BiTokens))
tdm2 <- removeSparseTerms(tdm2, 0.9999)
tdm2.freq <- sort(row_sums(tdm2, na.rm = T), decreasing=TRUE)
tdm2.freq <- data.frame(word=names(tdm2.freq), freq=tdm2.freq)

bi.plot <- as.data.frame(head(tdm2.freq, 10))
bi.plot$word <- factor(bi.plot$word,
                        levels=bi.plot$word[order(bi.plot$freq,
                                                   decreasing=TRUE)])

ggplot(bi.plot, aes(x=word, y=freq)) + geom_bar(stat="identity") + 
  xlab("Bi-grams in Corpus") + ylab("Count") + theme_minimal()
```

The above analysis can be repeated for other n-grams as well. Below is the frequency plot for tri-grams, which again makes sense with respect to the language. We can start to see the formation of common phrases, which is again a good sign for modeling, as this kind of n-gram approach to understanding the text is likely to form the basis of our predictive model. For example, is the text `one of` is provided to the eventual model, the data is clearly showing that the most likely next work is `the`. The challenge in the modeling phase will be in how to extend this kind of logic to words and pairings not found in the underlying Corpus.

```{r tritokenize, cache =TRUE, echo=FALSE, fig.align="center"}
tdm3 <- TermDocumentMatrix(docs, control=list(tokenize=TriTokens))
tdm3 <- removeSparseTerms(tdm3, 0.9999)
tdm3.freq <- sort(row_sums(tdm3, na.rm = T), decreasing=TRUE)
tdm3.freq <- data.frame(word=names(tdm3.freq), freq=tdm3.freq)

tri.plot <- as.data.frame(head(tdm3.freq, 10))
tri.plot$word <- factor(tri.plot$word,
                        levels=tri.plot$word[order(tri.plot$freq,
                                                   decreasing=TRUE)])

ggplot(tri.plot, aes(x=word, y=freq)) + geom_bar(stat="identity") + 
  xlab("Tri-grams in Corpus") + ylab("Count") + theme_minimal()
```