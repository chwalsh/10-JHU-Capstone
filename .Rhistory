install.packages("tm")
install.packages("tidyr")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("slam")
install.packages("Rweka")
install.packages("RWeka")
install.packages("data.table")
install.packages("hashFunction")
install.packages("hashmap")
## load required packages
require(tm)
require(tidyr)
require(dplyr)
require(ggplot2)
require(slam)
require(RWeka)
require(data.table)
require(hashFunction)
require(hashmap)
install.packages("Rjava")
install.packages("rJava")
require(rJava)
source("On Load.R")
UniGram <- readRDS("NGrams/Final_UniGram.rds")
BiGram <- readRDS("NGrams/Final_BiGram.rds")
source("On Load.R")
BiGram <- BiGram[j = list(freq_char = as.character(freq)), by = list(nmin1.gram, word, freq)]
BiFreqCount <- BiGram[j = list(count = count(freq_char)), by = freq_char]
BiFreqCount <- BiGram[j = list(freq_char, count = count(freq_char))]
BiFreqCount <- BiGram[j = list(freq_char)]
head(BiFreqCount)
BiFreqCount <- BiGram[j = list(freq_char, count = count(freq_char))]
?count
BiGram <- BiGram[j = list(freq_char = as.factor(freq)), by = list(nmin1.gram, word, freq)]
source("On Load.R")
BiGram <- BiGram[j = list(freq_grp = as.factor(freq)), by = list(nmin1.gram, word, freq)]
options(java.parameters = "- Xmx1024m")
install.packages("stringi")
library(stringi)
## set java heap size
options(java.parameters = "- Xmx1024m")
## load libraryd packages
library(tm)
library(tidyr)
library(dplyr)
library(ggplot2)
library(slam)
library(RWeka)
library(data.table)
# library(hashFunction)
# library(hashmap)
library(stringi)
twitter.con <- file("raw_data/final/en_US/en_US.twitter.txt", "r")
blogs.con <- file("raw_data/final/en_US/en_US.blogs.txt", "r")
news.con <- file("raw_data/final/en_US/en_US.news.txt", "rb")
US.twitter <- readLines(twitter.con, skipNul = TRUE)
US.blogs <- readLines(blogs.con, skipNul = TRUE)
US.news <- readLines(news.con, skipNul = TRUE)
set.seed(522)
US.twitter <- sample(US.twitter)
US.blogs <- sample(US.blogs)
US.news <- sample(US.news)
US.twitter.chunk <- split(US.twitter, cut(seq_along(US.twitter), 20, labels = FALSE))
US.blogs.chunk <- split(US.blogs, cut(seq_along(US.blogs), 20, labels = FALSE))
US.news.chunk <- split(US.news, cut(seq_along(US.news), 20, labels = FALSE))
close(twitter.con)
close(blogs.con)
close(news.con)
rm(twitter.con)
rm(blogs.con)
rm(news.con)
tail(UniGram)
UniGram$word <- stri_trans_general(UniGram$word, "latin-ascii")
tail(UniGram)
tail(UniGram, 30)
stri_trans_list()
UniGram$word <- stri_trans_general(stri_trans_general(UniGram$word, "any-latin"),"latin-ascii")
tail(UniGram, 30)
z™
x <- "zvyagintseva€™s"
x
stri_trans_general(x, "any-latin")
stri_trans_general(stri_trans_general(x, "any-latin"),"latin-ascii")
stri_trans_general(x, "latin-ascii")
stri_trans_general(x, "latin-ascii")
iconv(stri_trans_general(x, "latin-ascii"), "UTF-8", "ASCII", sub = "")
UniGram$word <- iconv(stri_trans_general(UniGram$word, "latin-ascii"), "UTF-8", "ASCII", sub = "")
tail(UniGram, 30)
i <- 1
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
doc.list <- lapply(doc.list, function(x) iconv(stri_trans_general(x, "latin-ascii"), "UTF-8", "ASCII", sub = ""))
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
head(doc.list)
doc.list <- lapply(doc.list, function(x) iconv(stri_trans_general(x, "latin-ascii"), "UTF-8", "ASCII", sub = ""))
head(doc.list)
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
head(doc.list)
doc.list <- lapply(doc.list, function(x) iconv(stri_trans_general(x, "latin-ascii"), "UTF-8", "ASCII", sub = ""))
head(doc.list)
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
## Clean Text
doc.list <- lapply(doc.list, function(x) iconv(x, "UTF-8", "ASCII", sub = ""))
head(doc.list)
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
head(doc.list)
doc.list <- lapply(doc.list, function(x) iconv(x, "UTF-8", "ASCII", sub = ""))
head(doc.list)
tail(UniGram)
UniGram <- readRDS("NGrams/Final_UniGram.rds")
tail((UniGram))
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
doc.list <- lapply(doc.list, function(x) stringi_conv(x, "UTF-8", "ASCII", sub = ""))
doc.list <- lapply(doc.list, function(x) stri_conv(x, "UTF-8", "ASCII")
)
warnings(50)
head(doc.list)
?stri_conv
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
doc.list <- lapply(doc.list, function(x) stri_conv(x, "latin", "ASCII"))
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
doc.list <- lapply(doc.list, function(x) stri_trans_general(x, "latin-ascii"))
head(doc.list)
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
doc.list <- lapply(doc.list, function(x) iconv(stri_trans_general(x, "latin-ascii"), "UTF-8", "ASCII", sub = ""))
head(doc.list)
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
doc.list <- lapply(doc.list, function(x) iconv(x, "UTF-8", "ASCII", sub = ""))
head(doc.list)
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
head(doc.list)
grepl("â",US.blogs)
US.blogs[grepl("â",US.blogs)]
?readLines
US.twitter <- readLines(twitter.con, skipNul = TRUE, encoding = "UTF-8")
twitter.con <- file("raw_data/final/en_US/en_US.twitter.txt", "r")
US.twitter <- readLines(twitter.con, skipNul = TRUE, encoding = "UTF-8")
US.twitter[grepl("â",US.twitter)]
US.twitter <- readLines(twitter.con, skipNul = TRUE, encoding = "latin-1")
US.twitter[grepl("â",US.twitter)]
US.twitter[grepl("Alice in Wonderland",US.twitter)]
head(US.twitter)
US.twitter <- readLines(twitter.con, skipNul = TRUE, encoding = "Latin-1")
close(twitter.con)
close(blogs.con)
close(news.con)
rm(twitter.con)
rm(blogs.con)
rm(news.con)
twitter.con <- file("raw_data/final/en_US/en_US.twitter.txt", "r")
blogs.con <- file("raw_data/final/en_US/en_US.blogs.txt", "r")
news.con <- file("raw_data/final/en_US/en_US.news.txt", "rb")
US.twitter <- readLines(twitter.con, skipNul = TRUE, encoding = "UTF-8")
US.blogs <- readLines(blogs.con, skipNul = TRUE, encoding = "UTF-8")
US.news <- readLines(news.con, skipNul = TRUE, encoding = "UTF-8")
set.seed(522)
US.twitter <- sample(US.twitter)
US.blogs <- sample(US.blogs)
US.news <- sample(US.news)
US.twitter.chunk <- split(US.twitter, cut(seq_along(US.twitter), 20, labels = FALSE))
US.blogs.chunk <- split(US.blogs, cut(seq_along(US.blogs), 20, labels = FALSE))
US.news.chunk <- split(US.news, cut(seq_along(US.news), 20, labels = FALSE))
close(twitter.con)
close(blogs.con)
close(news.con)
rm(twitter.con)
rm(blogs.con)
rm(news.con)
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
doc.list <- lapply(doc.list, function(x) stri_trans_general(x, "latin-ascii"))
doc.list <- lapply(doc.list, function(x) iconv(x, "UTF-8", "ASCII", sub = ""))
head(doc.list)
## Tokenize functions
BiTokens <- function(x) {NGramTokenizer(x, Weka_control(min = 2, max = 2))}
TriTokens <- function(x) {NGramTokenizer(x, Weka_control(min = 3, max = 3))}
QuadTokens <- function(x) {NGramTokenizer(x,Weka_control(min = 4, max = 4))}
for (i in 1:20){
doc.list <- list(blog = US.blogs.chunk[[i]], twitter = US.twitter.chunk[[i]],
news = US.news.chunk[[i]])
## Clean Text
doc.list <- lapply(doc.list, function(x) stri_trans_general(x, "latin-ascii"))
doc.list <- lapply(doc.list, function(x) iconv(x, "UTF-8", "ASCII", sub = ""))
docs <- VCorpus(VectorSource(doc.list))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
tdm <- TermDocumentMatrix(docs)
tdm <- removeSparseTerms(tdm, 0.9999)
tdm.freq <- sort(row_sums(tdm, na.rm = T), decreasing=TRUE)
rm(tdm)
tdm.freq <- data.table(word=names(tdm.freq), freq=tdm.freq)
saveRDS(tdm.freq, paste("NGrams/UniGram",i,".rds", sep = ""))
rm(tdm.freq)
# tdm2 <- TermDocumentMatrix(docs, control=list(tokenize=BiTokens))
# tdm2 <- removeSparseTerms(tdm2, 0.9999)
# tdm2.freq <- sort(row_sums(tdm2, na.rm = T), decreasing=TRUE)
# rm(tdm2)
# tdm2.freq <- data.table(word=names(tdm2.freq), freq=tdm2.freq)
# tdm2.freq <- tdm2.freq %>% separate(word, c("nmin1.gram", "word"), " ")
# tdm2.freq <- na.omit(tdm2.freq)
# saveRDS(tdm2.freq, paste("NGrams/BiGram",i,".rds", sep = ""))
# rm(tdm2.freq)
#
# tdm3 <- TermDocumentMatrix(docs, control=list(tokenize=TriTokens))
# tdm3 <- removeSparseTerms(tdm3, 0.9999)
# tdm3.freq <- sort(row_sums(tdm3, na.rm = T), decreasing=TRUE)
# rm(tdm3)
# tdm3.freq <- data.table(word=names(tdm3.freq), freq=tdm3.freq)
# tdm3.freq <- tdm3.freq %>% separate(word, c("one", "two", "word"), " ") %>%
#   unite(nmin1.gram, one:two, sep =" ")
# tdm3.freq <- na.omit(tdm3.freq)
# saveRDS(tdm3.freq, paste("NGrams/TriGram",i,".rds", sep = ""))
# rm(tdm3.freq)
#
# tdm4 <- TermDocumentMatrix(docs, control=list(tokenize=QuadTokens))
# tdm4 <- removeSparseTerms(tdm4, 0.9999)
# tdm4.freq <- sort(row_sums(tdm4, na.rm = T), decreasing=TRUE)
# rm(tdm4)
# tdm4.freq <- data.table(word=names(tdm4.freq), freq=tdm4.freq)
# tdm4.freq <- tdm4.freq %>% separate(word, c("one", "two", "three", "word"), " ") %>%
#   unite(nmin1.gram, one:three, sep =" ")
# tdm4.freq <- na.omit(tdm4.freq)
# saveRDS(tdm4.freq, paste("NGrams/QuadGram",i,".rds", sep = ""))
# rm(tdm4.freq)
}
i
US.twitter <- readLines(twitter.con, skipNul = TRUE, encoding = "UTF-8")
twitter.con <- file("raw_data/final/en_US/en_US.twitter.txt", "r")
US.twitter <- readLines(twitter.con, skipNul = TRUE, encoding = "UTF-8")
head(US.twitter)
US.twitter[grepl("Alice in Wonderland", US.twitter)]
stri_trans_general(US.twitter[grepl("Alice in Wonderland", US.twitter)], "latin-ascii"))
stri_trans_general(US.twitter[grepl("Alice in Wonderland", US.twitter)], "latin-ascii")
iconv(US.twitter[grepl("Alice in Wonderland", US.twitter)], "UTF-8", "ASCII", sub = "")
x
US.twitter[grepl("€", US.twitter)]
stri_trans_general(US.twitter[grepl("€", US.twitter)], "latin-ascii"))
stri_trans_general(US.twitter[grepl("€", US.twitter)], "latin-ascii")
iconv(US.twitter[grepl("€", US.twitter)], "UTF-8", "ASCII", sub = "")
rm(list=ls())
close(twitter.con)
close(blogs.con)
close(news.con)
BiGram <- readRDS("NGrams/Final_BiGram.rds")
head(BiGram)
twitter.con <- file("raw_data/final/en_US/en_US.twitter.txt", "r")
blogs.con <- file("raw_data/final/en_US/en_US.blogs.txt", "r")
news.con <- file("raw_data/final/en_US/en_US.news.txt", "rb")
close(twitter.con)
close(blogs.con)
close(news.con)
rm(twitter.con)
rm(blogs.con)
rm(news.con)
BiGram <- BiGram[j = list(freq_grp = as.factor(freq)), by = list(nmin1.gram, word, freq)]
head(BiGram)
